<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="what-is-light-gbm?">what is light gbm?<a class="anchor-link" href="#what-is-light-gbm?">¶</a></h1><p>LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework that uses tree-based learning algorithms. It is designed for distributed and efficient training, making it suitable for large datasets.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Why-is-LightGBM-faster?">Why is LightGBM faster?<a class="anchor-link" href="#Why-is-LightGBM-faster?">¶</a></h3><p>LightGBM achieves its speed and efficiency through several key innovations:</p>
<ul>
<li><strong>Histogram-based decision tree learning:</strong> Instead of looking at every possible split point, LightGBM buckets continuous feature values into discrete bins (histograms), which greatly reduces the computation needed for finding the best split.</li>
<li><strong>Leaf-wise tree growth:</strong> Unlike level-wise algorithms (which grow all branches evenly), LightGBM grows trees leaf-wise. It always splits the leaf with the largest value reduction, which can lead to deeper trees and better accuracy with fewer splits.</li>
<li><strong>Sparse feature optimization:</strong> LightGBM is designed to handle sparse data efficiently, skipping unnecessary computation on missing or zero values.</li>
<li><strong>Parallel and distributed computation:</strong> LightGBM can train across multiple CPU cores and even multiple machines, further improving scalability and training times.</li>
</ul>
<p>These techniques make LightGBM significantly faster than traditional implementations like XGBoost or scikit-learn's GradientBoosting.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Pros:">Pros:<a class="anchor-link" href="#Pros:">¶</a></h3><ul>
<li>High efficiency and speed: LightGBM is faster than many other boosting algorithms due to its histogram-based approach.</li>
<li>Scalability: It can handle large datasets and is capable of training on distributed systems.</li>
<li>Support for categorical features: LightGBM can directly handle categorical features without the need for one-hot encoding.</li>
<li>Flexibility: It offers various hyperparameters to tune, allowing for better model performance.</li>
</ul>
<h3 id="Cons:">Cons:<a class="anchor-link" href="#Cons:">¶</a></h3><ul>
<li>Complexity: The model can be complex to tune, requiring careful selection of hyperparameters.</li>
<li>Overfitting: LightGBM can overfit on small datasets if not properly regularized.</li>
<li>Less interpretability: Like many ensemble methods, the model can be less interpretable compared to simpler models.</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Generate a synthetic dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Split the dataset into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create a LightGBM dataset</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Set parameters for LightGBM</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'objective'</span><span class="p">:</span> <span class="s1">'binary'</span><span class="p">,</span>
    <span class="s1">'metric'</span><span class="p">:</span> <span class="s1">'binary_logloss'</span><span class="p">,</span>
    <span class="s1">'boosting_type'</span><span class="p">:</span> <span class="s1">'gbdt'</span><span class="p">,</span>
    <span class="s1">'num_leaves'</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="s1">'feature_fraction'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_binary</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">]</span>

<span class="c1"># Evaluate the model</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_binary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[LightGBM] [Info] Number of positive: 405, number of negative: 395
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001802 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 5100
[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 20
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.506250 -&gt; initscore=0.025001
[LightGBM] [Info] Start training from score 0.025001
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Accuracy: 0.93
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>