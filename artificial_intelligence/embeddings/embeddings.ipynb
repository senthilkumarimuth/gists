{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06a1ab0",
   "metadata": {},
   "source": [
    "# What are learned token embeddings?\n",
    "\n",
    "Learned token embeddings are vector representations of tokens (such as words, subwords, or characters) in a neural network model, where the embedding vectors are initialized randomly and then optimized (learned) during model training. \n",
    "\n",
    "These embeddings capture semantic and syntactic information about the tokens based on the training data, allowing the model to understand relationships and similarities between different tokens. \n",
    "\n",
    "Learned token embeddings are a foundational component in many natural language processing (NLP) models, such as word2vec, GloVe, and transformer-based architectures like BERT and GPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0ef62",
   "metadata": {},
   "source": [
    "#### Step-by-step explanation of learned token embeddings:\n",
    "\n",
    " 1. **Tokenization**: The input text is first split into smaller units called tokens (such as words, subwords, or characters).\n",
    " \n",
    " 2. **Embedding Initialization**: Each unique token is assigned a vector of numbers (embedding). These vectors are usually initialized randomly.\n",
    " \n",
    " 3. **Embedding Lookup**: When processing text, each token is replaced by its corresponding embedding vector, creating a sequence of vectors.\n",
    " \n",
    " 4. **Model Training**: As the neural network trains on data, the embedding vectors are updated through backpropagation. This means the model learns to adjust the vectors so that they capture useful information about the tokens.\n",
    " \n",
    " 5. **Semantic Representation**: After training, tokens with similar meanings or usage patterns have embedding vectors that are close together in the vector space. This helps the model understand relationships between tokens.\n",
    " \n",
    " 6. **Usage in NLP Tasks**: These learned embeddings are used as input features for various NLP tasks, such as text classification, translation, or question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80a862",
   "metadata": {},
   "source": [
    "# What is Sub word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8693e",
   "metadata": {},
   "source": [
    "Subword embeddings are a type of word representation in natural language processing where words are broken down into smaller units, such as character n-grams or subword tokens. This approach helps handle rare or unseen words by representing them as combinations of subword units, improving the model's ability to generalize and capture morphological patterns. Popular methods for generating subword embeddings include Byte Pair Encoding (BPE), WordPiece, and SentencePiece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfda8f0",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56488c6",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) is a subword tokenization technique used in natural language processing to efficiently represent words as sequences of more frequent subword units. BPE starts with a base vocabulary of individual characters and iteratively merges the most frequent pairs of symbols (characters or character sequences) in the training data to form new subword units. This process continues for a predefined number of merges, resulting in a vocabulary that can represent both common words and rare or unseen words as combinations of subword units. ***BPE helps models handle out-of-vocabulary words and capture meaningful word structure.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d180c4",
   "metadata": {},
   "source": [
    "### WordPiece\n",
    "\n",
    "WordPiece is a subword tokenization algorithm commonly used in natural language processing, especially in models like BERT. It works by breaking words into smaller, more frequent subword units based on their occurrence in a training corpus. The algorithm starts with a base vocabulary of characters and iteratively merges the most frequent pairs of symbols to form new subword tokens, similar to Byte Pair Encoding (BPE). WordPiece helps handle rare or unknown words by representing them as combinations of subword units, improving the model's ability to generalize and understand word structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c275862",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "\n",
    "SentencePiece is a text tokenization tool and algorithm that segments text into subword units, often used in natural language processing tasks. Unlike BPE and WordPiece, SentencePiece does not require pre-tokenized input and can operate directly on raw text, making it language-independent and suitable for languages without clear word boundaries. It uses algorithms like Unigram Language Model or BPE to learn a vocabulary of subword tokens from the training data. SentencePiece is widely used in models such as Google's T5 and ALBERT, enabling efficient handling of rare and unknown words by representing them as sequences of subword units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36675c00",
   "metadata": {},
   "source": [
    "### Character Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14867005",
   "metadata": {},
   "source": [
    "Character embeddings are a type of word representation in natural language processing where each character in a word is assigned a vector, and words are represented as sequences or combinations of these character vectors. This approach allows models to capture morphological patterns, handle misspellings, and process rare or unseen words by building word meaning from their constituent characters. Character embeddings are especially useful for languages with rich morphology or when dealing with noisy text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007831c5",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "Positional embeddings are a technique used in natural language processing models, especially in transformer architectures, to encode the order of tokens in a sequence. Since models like transformers process input tokens in parallel and lack inherent knowledge of token positions, positional embeddings inject information about the position of each token into the model. This is typically done by adding or concatenating a unique vector (the positional embedding) to each token embedding, allowing the model to capture the sequential structure of the input. Common approaches include fixed sinusoidal embeddings and learnable positional embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90654f30",
   "metadata": {},
   "source": [
    "## Difference between BPE, WordPiece, and SentencePiece\n",
    " \n",
    " | Feature                | BPE (Byte Pair Encoding) | WordPiece                | SentencePiece              |\n",
    " |------------------------|-------------------------|--------------------------|----------------------------|\n",
    " | **Algorithm**          | Greedy pair merging     | Greedy pair merging      | Unigram LM or BPE          |\n",
    " | **Input**              | Pre-tokenized text      | Pre-tokenized text       | Raw text (no pre-tokenization needed) |\n",
    " | **Merge Criteria**     | Most frequent pairs     | Most likely pairs (maximizes likelihood) | Probabilistic (Unigram LM) or frequent pairs (BPE) |\n",
    " | **Used in**            | GPT, RoBERTa            | BERT, DistilBERT         | T5, ALBERT, XLNet          |\n",
    " | **Language Independence** | Limited (needs tokenization) | Limited (needs tokenization) | High (works on raw text)   |\n",
    " | **Special Features**   | Simple, fast            | Handles unknowns with ## prefix | Can handle languages without spaces |\n",
    " \n",
    " **Summary:**\n",
    " - **BPE** and **WordPiece** both use iterative merging of frequent symbol pairs, but WordPiece optimizes for likelihood and often uses special prefixes for subwords.\n",
    " - **SentencePiece** can operate directly on raw text, supports multiple algorithms (Unigram LM, BPE), and is more language-independent.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
