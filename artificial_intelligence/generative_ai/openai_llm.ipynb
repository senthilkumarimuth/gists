{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model prompts often contain repetitive content, like system prompts and common instructions. OpenAI routes API requests to servers that recently processed the same prompt, making it cheaper and faster than processing a prompt from scratch. This can reduce latency by up to 80% and cost by 50% for long prompts. Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it.\n",
    "\n",
    "Prompt Caching is enabled for the following models:\n",
    "\n",
    "gpt-4o (excludes gpt-4o-2024-05-13 and chatgpt-4o-latest)\n",
    "gpt-4o-mini\n",
    "o1-preview\n",
    "o1-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Caching is enabled automatically for prompts that are 1024 tokens or longer. When you make an API request, the following steps occur:\n",
    "\n",
    "Cache Lookup: The system checks if the initial portion (prefix) of your prompt is stored in the cache.\n",
    "Cache Hit: If a matching prefix is found, the system uses the cached result. This significantly decreases latency and reduces costs.\n",
    "Cache Miss: If no matching prefix is found, the system processes your full prompt. After processing, the prefix of your prompt is cached for future requests.\n",
    "Cached prefixes generally remain active for 5 to 10 minutes of inactivity. However, during off-peak periods, caches may persist for up to one hour.\n",
    "\n",
    "\n",
    "More details can be found at https://platform.openai.com/docs/guides/prompt-caching"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
