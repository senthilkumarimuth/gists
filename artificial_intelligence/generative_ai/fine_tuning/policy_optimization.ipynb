{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3eb3064",
   "metadata": {},
   "source": [
    "## Policy Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08331b27",
   "metadata": {},
   "source": [
    " ### What is Policy Optimization in LLMs?\n",
    " \n",
    " Policy optimization in the context of large language models (LLMs) refers to the process of adjusting the model’s parameters (or \"policy\") to improve its behavior according to specific objectives. This is often done using techniques from reinforcement learning, where the LLM’s outputs (such as generated text) are evaluated and the model is updated to maximize positive outcomes—like alignment with human preferences, correctness, or safety.\n",
    " \n",
    " In large language models, policy optimization is a key stage in fine-tuning. Here, the \"policy\" dictates how the model generates responses given a prompt. By optimizing this policy with data from feedback, rewards, or preferences, we can direct the model to produce more desirable and appropriate outputs.\n",
    " \n",
    " Common methods for policy optimization in LLMs include algorithms like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), both of which align the model more closely with human values and expectations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dbf15",
   "metadata": {},
   "source": [
    " ### How does PPO work for LLMs?\n",
    "\n",
    " PPO (Proximal Policy Optimization) is widely used to fine-tune large language models (LLMs) to align them with human preferences, commonly in RLHF (Reinforcement Learning from Human Feedback). Here’s how it works:\n",
    "\n",
    " 1. **Generate responses:** The LLM (policy) generates text responses to input prompts.\n",
    " 2. **Evaluate responses:** Each response is evaluated—often using a reward model trained on human feedback, or directly with human preference comparisons.\n",
    " 3. **Policy update:** PPO is used to update the LLM's parameters to maximize the expected reward, while ensuring the new policy stays close to the original (pretrained) policy. The \"clipping\" mechanism prevents excessively large updates that might degrade the model’s performance or lead to unwanted behaviors.\n",
    " 4. **Iterate:** This process is repeated: sampling outputs, evaluating, and updating, gradually improving the LLM’s outputs according to the reward model and alignment objectives.\n",
    "\n",
    " PPO’s reliability and stability make it the standard approach for optimizing the behavior of LLMs in safety-critical or alignment-conscious systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863f350",
   "metadata": {},
   "source": [
    " ### How does DPO work for LLMs?\n",
    "\n",
    " DPO (Direct Preference Optimization) is a recent method for fine-tuning large language models (LLMs) directly using preference data, such as rankings or pairwise human feedback. Unlike traditional RLHF approaches, DPO can optimize LLMs to align with human preferences without relying on explicit reward modeling or reinforcement learning.\n",
    "\n",
    " Here’s how DPO typically works for LLMs:\n",
    "\n",
    " 1. **Collect preferences:** Gather data where annotators have compared two (or more) model-generated responses and indicated which they prefer in each case.\n",
    " 2. **Formulate the optimization:** DPO frames the objective so that the model is directly optimized to prefer human-chosen responses over less preferred ones, often via a contrastive loss.\n",
    " 3. **Update the model:** The LLM’s parameters are updated so that, given the same prompt, the log-probability of preferred responses is increased relative to non-preferred ones, subject to a regularization constraint to stay close to the base model.\n",
    " 4. **Iterate:** This process is repeated, with the model progressively better aligning its generations to human preferences without the complexity of policy optimization or reward models.\n",
    "\n",
    " Compared to PPO-based RLHF, DPO is often simpler, more stable, and computationally efficient, making it an attractive option for preference-based LLM fine-tuning.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
