{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162e64bd",
   "metadata": {},
   "source": [
    "## Why Scaling?\n",
    "\n",
    "Scaling is an essential step in data preprocessing for machine learning. Different features in a dataset may have different units, scales, or ranges (for example, age may range from 0–100, whereas income can range from thousands to millions). When features are on different scales, many machine learning algorithms may perform poorly or take much longer to converge because:\n",
    "\n",
    "- Algorithms that use distances (such as k-nearest neighbors, K-means clustering, and support vector machines) can be dominated by features with larger numerical values.\n",
    "- Algorithms that use gradient-based optimization (such as neural networks and logistic regression) may take longer to train because features of varying scales can lead to unstable gradients.\n",
    "- Many machine learning models assume that all features are centered around zero and have equal variance.\n",
    "\n",
    "Scaling ensures that the features contribute equally to the result, speeding up learning and improving model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33911d",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "Standard Scaling (also called z-score normalization) is a preprocessing technique used to standardize the features of your data to given range (-1,1) so that they have the properties of a standard normal distribution (mean = 0 and standard deviation = 1).\n",
    "\n",
    "You can apply standard scaling using Scikit-learn's `StandardScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your data matrix\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52410b",
   "metadata": {},
   "source": [
    "#### formula:\n",
    " \n",
    " z = (x - μ) / σ\n",
    " \n",
    " Where:\n",
    " - x: original feature value\n",
    " - μ: mean of the feature\n",
    " - σ: standard deviation of the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef73a75",
   "metadata": {},
   "source": [
    " ### Pros and Cons of Standard Scaling\n",
    " \n",
    " **Pros:**\n",
    " - Helps many machine learning algorithms (like SVM, KNN, logistic regression) perform better by normalizing the range of features.\n",
    " - Accelerates convergence when training neural networks.\n",
    " - Reduces the impact of features with larger scales, making models less sensitive to feature scaling.\n",
    " \n",
    " **Cons:**\n",
    " - Sensitive to outliers, since the mean and standard deviation can be affected by extreme values.\n",
    " - The transformed data loses its original units, which may reduce interpretability.\n",
    " - Not always necessary for tree-based algorithms (like Random Forest or Decision Trees), where scaling usually doesn't impact performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5097df",
   "metadata": {},
   "source": [
    " ## Min-Max Normalization\n",
    " Min-Max Normalization (also known as Min-Max scaling) is a technique used to transform features to a given range, typically [0, 1]. This ensures that all features have the same scale, which can improve the performance of many machine learning algorithms.\n",
    "\n",
    " You can apply Min-Max Normalization using Scikit-learn's `MinMaxScaler`:\n",
    "\n",
    " ```python\n",
    " from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " scaler = MinMaxScaler()\n",
    " X_normalized = scaler.fit_transform(X)  # X is your data matrix\n",
    " ```\n",
    "\n",
    " #### Formula:\n",
    "\n",
    "  x_norm = (x - min) / (max - min)\n",
    "\n",
    " Where:\n",
    " - x: original feature value\n",
    " - min: minimum value of the feature\n",
    " - max: maximum value of the feature\n",
    "\n",
    " ### Pros and Cons of Min-Max Normalization\n",
    "\n",
    " **Pros:**\n",
    " - Scales all features to exactly fall within the specified range, commonly [0, 1].\n",
    " - Useful for algorithms that require bounded inputs (like neural networks with sigmoid activation).\n",
    " - Preserves the shape of the original data distribution.\n",
    "\n",
    " **Cons:**\n",
    " - Sensitive to outliers since min and max values can be affected by extreme data points.\n",
    " - Might squash the range of the data, causing information loss if outliers are present.\n",
    " - Not suitable if the test data contains new min or max values not seen in the training data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
