{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed33911d",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "Standard Scaling (also called z-score normalization) is a preprocessing technique used to standardize the features of your data to given range (-1,1) so that they have the properties of a standard normal distribution (mean = 0 and standard deviation = 1).\n",
    "\n",
    "You can apply standard scaling using Scikit-learn's `StandardScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your data matrix\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52410b",
   "metadata": {},
   "source": [
    "#### formula:\n",
    " \n",
    " z = (x - μ) / σ\n",
    " \n",
    " Where:\n",
    " - x: original feature value\n",
    " - μ: mean of the feature\n",
    " - σ: standard deviation of the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef73a75",
   "metadata": {},
   "source": [
    " ### Pros and Cons of Standard Scaling\n",
    " \n",
    " **Pros:**\n",
    " - Helps many machine learning algorithms (like SVM, KNN, logistic regression) perform better by normalizing the range of features.\n",
    " - Accelerates convergence when training neural networks.\n",
    " - Reduces the impact of features with larger scales, making models less sensitive to feature scaling.\n",
    " \n",
    " **Cons:**\n",
    " - Sensitive to outliers, since the mean and standard deviation can be affected by extreme values.\n",
    " - The transformed data loses its original units, which may reduce interpretability.\n",
    " - Not always necessary for tree-based algorithms (like Random Forest or Decision Trees), where scaling usually doesn't impact performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5097df",
   "metadata": {},
   "source": [
    " ## Min-Max Normalization\n",
    " Min-Max Normalization (also known as Min-Max scaling) is a technique used to transform features to a given range, typically [0, 1]. This ensures that all features have the same scale, which can improve the performance of many machine learning algorithms.\n",
    "\n",
    " You can apply Min-Max Normalization using Scikit-learn's `MinMaxScaler`:\n",
    "\n",
    " ```python\n",
    " from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " scaler = MinMaxScaler()\n",
    " X_normalized = scaler.fit_transform(X)  # X is your data matrix\n",
    " ```\n",
    "\n",
    " #### Formula:\n",
    "\n",
    "  x_norm = (x - min) / (max - min)\n",
    "\n",
    " Where:\n",
    " - x: original feature value\n",
    " - min: minimum value of the feature\n",
    " - max: maximum value of the feature\n",
    "\n",
    " ### Pros and Cons of Min-Max Normalization\n",
    "\n",
    " **Pros:**\n",
    " - Scales all features to exactly fall within the specified range, commonly [0, 1].\n",
    " - Useful for algorithms that require bounded inputs (like neural networks with sigmoid activation).\n",
    " - Preserves the shape of the original data distribution.\n",
    "\n",
    " **Cons:**\n",
    " - Sensitive to outliers since min and max values can be affected by extreme data points.\n",
    " - Might squash the range of the data, causing information loss if outliers are present.\n",
    " - Not suitable if the test data contains new min or max values not seen in the training data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
