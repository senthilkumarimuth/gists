{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Doc2Vec?\n",
    "\n",
    "Doc2Vec is an extension of the Word2Vec algorithm, designed to create vector representations (embeddings) of entire documents rather than just individual words. It was introduced by Quoc Le and Tomas Mikolov in 2014.\n",
    "\n",
    "Key features of Doc2Vec:\n",
    "1. Document-level embeddings: It can generate fixed-length feature vectors for variable-length pieces of text, such as sentences, paragraphs, or entire documents.\n",
    "2. Unsupervised learning: It learns from unlabeled text data.\n",
    "3. Semantic understanding: The resulting embeddings capture semantic similarities between documents.\n",
    "4. Versatility: It can be used for various NLP tasks like document classification, clustering, and information retrieval.\n",
    "\n",
    "Doc2Vec has two main training algorithms:\n",
    "1. Distributed Memory (DM)\n",
    "2. Distributed Bag of Words (DBOW)\n",
    "\n",
    "These embeddings can be used as features for machine learning models or for measuring document similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Memory (DM)\n",
    "\n",
    "The Distributed Memory (DM) is one of the two main approaches used in Doc2Vec, alongside the Distributed Bag of Words (DBOW). In this model:\n",
    "\n",
    "1. The document vector is trained to predict the next word given both the document vector and a context of words.\n",
    "2. It's similar to the Continuous Bag of Words (CBOW) model in Word2Vec, but with an additional document vector.\n",
    "3. DM preserves word order information within a small context window.\n",
    "4. It often produces more accurate results than DBOW, especially on larger datasets.\n",
    "5. The DM model can be computationally more expensive than DBOW.\n",
    "\n",
    "The DM approach in Doc2Vec allows for a richer representation of documents, capturing both the semantic meaning and some aspects of word order, which can be beneficial for various NLP tasks such as document classification and information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Bag of Words (DBOW)\n",
    "\n",
    "The Distributed Bag of Words (DBOW) is one of the two main approaches used in Doc2Vec. In this model:\n",
    "\n",
    "1. The document vector is trained to predict the words in the document.\n",
    "2. It's similar to the Skip-gram model in Word2Vec, but instead of using a word to predict surrounding words, it uses the document vector to predict words in the document.\n",
    "3. DBOW is generally faster and uses less memory than the Distributed Memory (DM) approach.\n",
    "4. It tends to perform well on small datasets and is less prone to overfitting.\n",
    "\n",
    "The DBOW model in Doc2Vec captures the overall semantic meaning of the document, allowing for efficient document similarity comparisons and other NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for new document: [-7.3994203e-03  5.6641987e-03 -5.2486341e-03  8.0357371e-03\n",
      "  3.5742640e-03  4.0831073e-04 -6.3471780e-03  4.2954260e-03\n",
      "  7.9677692e-03 -2.5655502e-03  1.0256020e-03  6.1734598e-03\n",
      "  3.4994003e-03  2.0215532e-03  1.2132036e-04 -1.0053399e-02\n",
      " -6.3982286e-04 -9.6847489e-03 -7.3886719e-03  7.2929231e-03\n",
      " -3.8932411e-03  4.4750838e-04 -3.4880273e-03  2.9103050e-03\n",
      "  6.3013220e-03  6.6286274e-03  9.2212204e-03 -4.8543504e-03\n",
      " -6.3678785e-03 -5.2491724e-03 -4.9779154e-03 -8.0104759e-03\n",
      " -2.5848330e-03 -9.3911439e-03  8.8495361e-03 -3.8854422e-03\n",
      "  8.9726066e-03  6.0794423e-03 -3.5426712e-03 -1.9116531e-03\n",
      " -9.6985502e-03 -9.8396381e-03 -6.1360551e-03  9.6534677e-03\n",
      "  9.4779823e-03  2.5378338e-03  9.5006726e-05 -8.8074617e-03\n",
      "  6.2138718e-03 -6.3329767e-03]\n",
      "\n",
      "Similar documents:\n",
      "Document 3: Similarity = 0.1360\n",
      "Document 0: Similarity = 0.0904\n",
      "Document 1: Similarity = -0.0176\n",
      "Document 2: Similarity = -0.0890\n",
      "Document 4: Similarity = -0.2382\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Doc2Vec is an extension of Word2Vec for entire documents.\",\n",
    "    \"It creates vector representations of variable-length text.\",\n",
    "    \"The algorithm can capture semantic similarities between documents.\",\n",
    "    \"Doc2Vec is useful for various NLP tasks like classification and clustering.\",\n",
    "    \"It uses unsupervised learning to generate document embeddings.\"\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Example usage: Infer vector for a new document\n",
    "new_doc = \"Doc2Vec is used for document embeddings\"\n",
    "new_vector = model.infer_vector(word_tokenize(new_doc.lower()))\n",
    "\n",
    "print(\"Vector for new document:\", new_vector)\n",
    "\n",
    "# Find similar documents\n",
    "similar_docs = model.dv.most_similar([new_vector])\n",
    "print(\"\\nSimilar documents:\")\n",
    "for doc, similarity in similar_docs:\n",
    "    print(f\"Document {doc}: Similarity = {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
