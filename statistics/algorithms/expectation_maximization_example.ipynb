{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization (EM) Algorithm\n",
    "\n",
    "The Expectation Maximization (EM) algorithm is a powerful iterative method used for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, especially when the model depends on unobserved latent variables. It is widely used in machine learning, data mining, and statistical analysis.\n",
    "\n",
    "Key features of the EM algorithm:\n",
    "\n",
    "1. It's particularly useful for problems involving incomplete or missing data.\n",
    "2. The algorithm alternates between two steps: Expectation (E) step and Maximization (M) step.\n",
    "3. It's guaranteed to converge to a local optimum.\n",
    "\n",
    "The EM algorithm consists of two main steps:\n",
    "\n",
    "1. Expectation (E) step: Estimate the expected value of the log-likelihood function, using the current estimate for the parameters.\n",
    "\n",
    "2. Maximization (M) step: Find the parameter values that maximize the expected log-likelihood found in the E step.\n",
    "\n",
    "These steps are repeated until the algorithm converges to a local maximum of the likelihood function.\n",
    "\n",
    "Common applications of the EM algorithm include:\n",
    "- Gaussian Mixture Models\n",
    "- Hidden Markov Models\n",
    "- Latent variable models in general\n",
    "\n",
    "In the following notebook, we'll implement a simple example of the EM algorithm to demonstrate its workings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated means: [ 7.20001575 12.80003149]\n",
      "Filled data: [ 2.         10.00007873  6.          8.         10.00007873 10.\n",
      " 12.         14.         10.00007873 18.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some example data with missing values\n",
    "np.random.seed(42)\n",
    "X = np.array([2.0, np.nan, 6.0, 8.0, np.nan, 10.0, 12.0, 14.0, np.nan, 18.0])\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_parameters_with_missing_data(X, k):\n",
    "    means = np.random.choice(X[~np.isnan(X)], k)  # randomly choose means from observed values\n",
    "    return means\n",
    "\n",
    "# E-step: Fill in missing values with current mean estimates\n",
    "def expectation_step_with_missing_data(X, means):\n",
    "    X_filled = X.copy()\n",
    "    for i in range(len(X)):\n",
    "        if np.isnan(X[i]):\n",
    "            X_filled[i] = means.mean()  # Replace missing values with the mean of means\n",
    "    return X_filled\n",
    "\n",
    "# M-step: Update means based on the filled data\n",
    "def maximization_step_with_missing_data(X_filled, k):\n",
    "    clusters = np.array_split(np.sort(X_filled), k)  # Simple 1D clustering based on sorting\n",
    "    means = np.array([np.mean(cluster) for cluster in clusters])\n",
    "    return means\n",
    "\n",
    "# Example usage\n",
    "k = 2  # Number of clusters\n",
    "means = initialize_parameters_with_missing_data(X, k)\n",
    "\n",
    "for _ in range(10):  # Iterate EM algorithm\n",
    "    X_filled = expectation_step_with_missing_data(X, means)\n",
    "    means = maximization_step_with_missing_data(X_filled, k)\n",
    "\n",
    "print(\"Estimated means:\", means)\n",
    "print(\"Filled data:\", X_filled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
