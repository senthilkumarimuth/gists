{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac69f73d",
   "metadata": {},
   "source": [
    "## Cosine Distance\n",
    "\n",
    "The **cosine distance** is a measure of dissimilarity between two non-zero vectors of an inner product space. It is derived from the cosine similarity, which measures the cosine of the angle between two vectors.\n",
    "\n",
    "- **Cosine similarity** is defined as:\n",
    "\n",
    "  $$\n",
    "  \\text{cosine similarity} = \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "  $$\n",
    "\n",
    "- **Cosine distance** is:\n",
    "\n",
    "  $$\n",
    "  \\text{cosine distance} = 1 - \\text{cosine similarity}\n",
    "  $$\n",
    "\n",
    "Cosine distance ranges from 0 (identical direction) to 2 (opposite direction), but for non-negative data, it typically ranges from 0 to 1.\n",
    "\n",
    "Cosine distance is commonly used in text analysis and information retrieval, where the magnitude of the vectors may not be as important as their orientation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82c530",
   "metadata": {},
   "source": [
    " #### Pros and Cons of Cosine Distance\n",
    " \n",
    " **Pros:**\n",
    " - **Insensitive to Magnitude:** Cosine distance focuses on the orientation (direction) of vectors, making it useful when the magnitude is not important, such as in text analysis (e.g., word counts).\n",
    " - **Effective for Sparse Data:** It works well with high-dimensional, sparse data, which is common in information retrieval and natural language processing.\n",
    " - **Scale Invariance:** Since it measures the angle between vectors, it is not affected by the scale of the data.\n",
    " \n",
    " **Cons:**\n",
    " - **Ignores Magnitude:** If the magnitude of vectors carries important information, cosine distance may not be appropriate.\n",
    " - **Not a True Metric:** Cosine distance does not always satisfy the triangle inequality, so it is not a true metric in the mathematical sense.\n",
    " - **Requires Non-zero Vectors:** It cannot be computed if one or both vectors are zero.\n",
    " - **Less Interpretable for Negative Values:** When data contains negative values, interpreting cosine distance can be less straightforward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9d28c",
   "metadata": {},
   "source": [
    " ## Euclidean Distance\n",
    " \n",
    " The **Euclidean distance** is the straight-line distance between two points in Euclidean space. It is the most common distance metric and is derived from the Pythagorean theorem.\n",
    " \n",
    " - **Euclidean distance** between two points $\\mathbf{A} = (a_1, a_2, ..., a_n)$ and $\\mathbf{B} = (b_1, b_2, ..., b_n)$ is defined as:\n",
    " \n",
    "   $$\n",
    "   d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\cdots + (a_n - b_n)^2}\n",
    "   $$\n",
    " \n",
    " Euclidean distance is always non-negative and equals zero only when the two points are identical.\n",
    " \n",
    " It is widely used in clustering, classification, and many other machine learning and statistical applications.\n",
    " \n",
    " #### Pros and Cons of Euclidean Distance\n",
    " \n",
    " **Pros:**\n",
    " - **Intuitive:** It corresponds to our everyday notion of physical distance.\n",
    " - **Metric Properties:** It satisfies all the properties of a metric (non-negativity, identity, symmetry, triangle inequality).\n",
    " - **Widely Applicable:** Useful in many algorithms, such as k-nearest neighbors and k-means clustering.\n",
    " \n",
    " **Cons:**\n",
    " - **Sensitive to Scale:** Features with larger scales can dominate the distance; data often needs to be normalized.\n",
    " - **Affected by Outliers:** Outliers can have a large impact on the distance.\n",
    " - **Not Suitable for Categorical Data:** It is only meaningful for continuous numerical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cdd2e",
   "metadata": {},
   "source": [
    "## Chi-Square Distance\n",
    "\n",
    "The **Chi-Square distance** is a measure commonly used to compare two binned distributions or histograms, especially in fields like image analysis and text mining. It is particularly useful when comparing frequency counts or probability distributions.\n",
    "\n",
    "- **Chi-Square distance** between two vectors $\\mathbf{A} = (a_1, a_2, ..., a_n)$ and $\\mathbf{B} = (b_1, b_2, ..., b_n)$ is defined as:\n",
    "\n",
    "  $$\n",
    "  d(\\mathbf{A}, \\mathbf{B}) = \\frac{1}{2} \\sum_{i=1}^n \\frac{(a_i - b_i)^2}{a_i + b_i}\n",
    "  $$\n",
    "\n",
    "  where the sum is taken over all bins where $a_i + b_i > 0$.\n",
    "\n",
    "The Chi-Square distance is always non-negative and equals zero only when the two distributions are identical.\n",
    "\n",
    "It is widely used for comparing histograms, contingency tables, and other types of frequency data.\n",
    "\n",
    "#### Pros and Cons of Chi-Square Distance\n",
    "\n",
    "**Pros:**\n",
    "- **Sensitive to Distribution Differences:** Highlights differences in relative frequencies between distributions.\n",
    "- **Useful for Histograms:** Well-suited for comparing binned data or categorical distributions.\n",
    "- **Scale Normalization:** The denominator normalizes for the scale of the counts.\n",
    "\n",
    "**Cons:**\n",
    "- **Undefined for Zero Sums:** If $a_i + b_i = 0$ for any bin, the term is undefined (usually skipped).\n",
    "- **Sensitive to Small Counts:** Can be unstable when counts are very small or zero.\n",
    "- **Not a True Metric:** Does not always satisfy the triangle inequality.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
